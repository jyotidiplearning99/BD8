# src/data_loader.py
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import os
import warnings
import cv2
from pathlib import Path
from typing import Dict, Optional
import tifffile

class BD_S8_Dataset(Dataset):
    """Dataset for BD S8 - Works with CVW files"""
    
    def __init__(self, 
                 excel_path: str,
                 data_folder: str,
                 transform: Optional[A.Compose] = None,
                 mode: str = 'train'):
        
        self.metadata = pd.read_excel(excel_path)
        self.data_folder = Path(data_folder)
        self.transform = transform
        self.mode = mode
        
        # Filter valid samples based on AB stained
        self.metadata = self.metadata[self.metadata['AB stained=1'] == 1]
        
        # Find image files - look in both main folder and converted folder
        self.image_files = self._find_image_files()
        
        print(f"âœ… Dataset initialized")
        print(f"ðŸ“Š Found {len(self.metadata)} AB-stained samples in Excel")
        print(f"ðŸ“ Found {len(self.image_files)} image files")
        if self.image_files:
            print(f"ðŸ“ File types: {set([f.suffix for f in self.image_files])}")
        
    def _find_image_files(self):
        """Find all image files in the data folder and converted subfolder"""
        files = []
        
        # Check main data folder
        for pattern in ['*.cvw', '*.czi', '*.tif', '*.tiff', '*.png']:
            files.extend(self.data_folder.glob(pattern))
        
        # Also check converted subfolder
        converted_folder = self.data_folder / 'converted'
        if converted_folder.exists():
            for pattern in ['*.tif', '*.tiff', '*.png']:
                files.extend(converted_folder.glob(pattern))
        
        return sorted(files)
    
    def __len__(self):
        return len(self.metadata)
    
    def __getitem__(self, idx):
        row = self.metadata.iloc[idx]
        
        # Load image using the safe loader - ALWAYS returns float32
        img_data = self.load_image_safe(idx)
        
        # Ensure float32 even if no transform
        img_data = img_data.astype(np.float32)
        
        # Create labels from metadata
        labels = {
            'extraction_method': 0 if row['Extraction method'] == 'Ficoll' else 1,
            'fresh_frozen': int(row['Fresh=1, frozen=0']),
            'fixed': int(row['Fixed=1']),
            'permeabilized': int(row['Permeabilized=1']),
            'viability': int(row['Viability dye=1'])
        }
        
        # Apply transformations
        if self.transform:
            augmented = self.transform(image=img_data)
            img_data = augmented['image']
        
        return {
            'image': img_data,
            'labels': labels,
            'metadata': row.to_dict()
        }
    
    def load_image_safe(self, idx: int) -> np.ndarray:
        """Load image with fallback to synthetic data"""
        
        # If no image files found, generate synthetic
        if not self.image_files:
            if self.mode == 'train':
                print(f"âš ï¸ No image files found, using synthetic data")
            return self.generate_synthetic_image(idx)
        
        # Get the file for this index
        file_idx = idx % len(self.image_files)
        image_path = self.image_files[file_idx]
        
        # Only print in test mode
        if self.mode == 'test':
            print(f"ðŸ“ Attempting to load: {image_path.name}")
        
        # Handle different file types
        if image_path.suffix == '.cvw':
            # For CVW files, check if there's a converted TIFF version
            converted_path = self.data_folder / 'converted' / (image_path.stem + '.tif')
            if converted_path.exists():
                if self.mode == 'test':
                    print(f"  âœ… Found converted TIFF: {converted_path.name}")
                return self.load_tiff(converted_path)
            else:
                if self.mode == 'test':
                    print(f"  âš ï¸ CVW file found but no converted TIFF. Using synthetic data.")
                    print(f"  ðŸ’¡ To convert, install Java and use: bfconvert {image_path} {converted_path}")
                return self.generate_synthetic_image(idx)
                
        elif image_path.suffix in ['.tif', '.tiff']:
            # Load TIFF files directly
            return self.load_tiff(image_path)
            
        else:
            if self.mode == 'test':
                print(f"  âš ï¸ Unknown file format: {image_path.suffix}")
            return self.generate_synthetic_image(idx)
    
    def load_tiff(self, path: Path) -> np.ndarray:
        """Load and process TIFF file"""
        try:
            if self.mode == 'test':
                print(f"  Loading TIFF: {path.name}")
            img = tifffile.imread(path)
            
            # Process image to ensure correct format
            img = self.process_image(img)
            
            # Apply spillover correction
            img = self.correct_spillover(img)
            
            return img.astype(np.float32)
            
        except Exception as e:
            print(f"  âŒ Error loading TIFF: {e}")
            return self.generate_synthetic_image(0)
    
    def process_image(self, img: np.ndarray) -> np.ndarray:
        """Process loaded image to correct format (512x512x3)"""
        # Ensure 3 channels
        if img.ndim == 2:
            # Single channel - replicate to 3
            img = np.stack([img] * 3, axis=-1)
        elif img.ndim == 3:
            if img.shape[0] <= 4:  # CHW format
                img = np.transpose(img[:3], (1, 2, 0))
            elif img.shape[-1] > 3:  # HWC with many channels
                img = img[:, :, :3]
            # else assume HWC with 3 channels
        
        # Resize if needed
        if img.shape[:2] != (512, 512):
            img_resized = np.zeros((512, 512, 3), dtype=np.float32)
            for c in range(min(3, img.shape[-1])):
                img_resized[:, :, c] = cv2.resize(img[:, :, c].astype(np.float32), (512, 512))
            img = img_resized
        
        # Normalize to [0, 1]
        img = img.astype(np.float32)
        for c in range(min(3, img.shape[-1])):
            channel = img[..., c]
            p1, p99 = np.percentile(channel, (1, 99))
            if p99 > p1:
                img[..., c] = np.clip((channel - p1) / (p99 - p1), 0, 1)
        
        return img.astype(np.float32)
    
    def correct_spillover(self, img: np.ndarray) -> np.ndarray:
        """Apply spillover correction based on your Excel notes"""
        if img.shape[-1] != 3:
            return img
        
        # Spillover matrix from your Excel (OBS! Fluorescence spillover noted)
        spillover_matrix = np.array([
            [1.0, 0.15, 0.05],   # CTSG (FITC) spillover
            [0.10, 1.0, 0.08],   # MPO (PE) spillover
            [0.02, 0.05, 1.0]    # HLA_ABC (RB780) spillover
        ], dtype=np.float32)
        
        # Apply correction
        img_flat = img.reshape(-1, 3).T
        try:
            corrected = np.linalg.solve(spillover_matrix, img_flat)
            corrected = np.clip(corrected.T, 0, 1).reshape(img.shape)
            return corrected.astype(np.float32)
        except:
            return img.astype(np.float32)
    
    def generate_synthetic_image(self, idx: int) -> np.ndarray:
        """Generate synthetic microscopy-like image based on your data"""
        # Use idx for different random seeds
        np.random.seed(42 + idx)
        
        # Create 512x512x3 image - ENSURE FLOAT32 from start
        img = np.zeros((512, 512, 3), dtype=np.float32)
        
        # Add synthetic cells with fluorescence patterns
        n_cells = np.random.randint(15, 40)
        
        for i in range(n_cells):
            # Random position and size
            center_x = np.random.randint(50, 462)
            center_y = np.random.randint(50, 462)
            radius = np.random.randint(10, 25)
            
            # Create meshgrid for this cell
            y, x = np.ogrid[:512, :512]
            mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
            
            # Different intensities for each marker (based on your data)
            # CTSG (FITC) - Channel 0
            if np.random.rand() > 0.3:  # 70% positive
                intensity_fitc = np.float32(np.random.uniform(0.3, 0.8))
                img[:, :, 0][mask] = np.maximum(img[:, :, 0][mask], intensity_fitc)
            
            # MPO (PE) - Channel 1  
            if np.random.rand() > 0.4:  # 60% positive
                intensity_pe = np.float32(np.random.uniform(0.2, 0.7))
                img[:, :, 1][mask] = np.maximum(img[:, :, 1][mask], intensity_pe)
            
            # HLA_ABC (RB780) - Channel 2
            if np.random.rand() > 0.2:  # 80% positive
                intensity_rb780 = np.float32(np.random.uniform(0.4, 0.9))
                img[:, :, 2][mask] = np.maximum(img[:, :, 2][mask], intensity_rb780)
        
        # Add background and noise - ensure float32
        background = (np.random.randn(512, 512, 3) * 0.02 + 0.05).astype(np.float32)
        img = np.clip(img + background, 0, 1).astype(np.float32)
        
        # Apply Gaussian blur for realistic appearance
        for c in range(3):
            img[:, :, c] = cv2.GaussianBlur(img[:, :, c], (5, 5), 1)
        
        # Final ensure float32
        return img.astype(np.float32)

def get_transforms(mode: str = 'train') -> A.Compose:
    """Get augmentation transforms"""
    if mode == 'train':
        return A.Compose([
            A.RandomRotate90(p=0.5),
            A.Flip(p=0.5),
            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),
            A.GaussNoise(var_limit=(0.01, 0.05), p=0.2),
            A.Resize(512, 512),
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.Resize(512, 512),
            ToTensorV2()
        ])

def get_dataloader(excel_path: str, 
                   data_folder: str,
                   batch_size: int = 4,
                   mode: str = 'train',
                   num_workers: int = 2) -> DataLoader:
    """Create dataloader for BD S8 data"""
    
    dataset = BD_S8_Dataset(
        excel_path=excel_path,
        data_folder=data_folder,
        transform=get_transforms(mode),
        mode=mode
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(mode == 'train'),
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available()
    )
    
    return dataloader
